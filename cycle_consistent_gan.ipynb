{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66884a53dee4f4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:35.878012300Z",
     "start_time": "2023-08-01T16:58:32.328525400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc7af56f68f2441b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:35.898071400Z",
     "start_time": "2023-08-01T16:58:35.880605Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff110117d99da5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:37.793316700Z",
     "start_time": "2023-08-01T16:58:37.763638200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_ROOT_DIR = 'dataset/images' # change to google drive path\n",
    "SAMPLES_DIR = 'dataset/samples' # change to google drive path\n",
    "TRAIN_A = 'trainA'\n",
    "TRAIN_B = 'trainB'\n",
    "TEST_A = 'testA'\n",
    "TEST_B = 'testB'\n",
    "UNSEEN_DEMO_IMAGES = 'dataset/unseen_demo_images' # change to google drive path\n",
    "UNSEEN_CT = 'ct'\n",
    "UNSEEN_MRI = 'mri'\n",
    "DATASET_TRAIN_MODE = 'train'\n",
    "DATASET_TEST_MODE = 'test'\n",
    "SHUFFLE = True\n",
    "EPOCHS = 0\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "VAL_BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.0002\n",
    "DECAY_START_EPOCH = 100\n",
    "BETA_1 = 0.5\n",
    "BETA_2 = 0.999\n",
    "N_CPU = 2  # change to a number based on the device for the training (8)\n",
    "IMG_SIZE = 128\n",
    "CHANNELS = 3\n",
    "N_CRITIC = 5\n",
    "SAMPLE_INTERVAL = 100\n",
    "NUM_RESIDUAL_BLOCKS = 19\n",
    "LAMBDA_CYC = 10.0\n",
    "LAMBDA_ID = 5.0\n",
    "NUM_WORKERS = 1\n",
    "\n",
    "ACTIVATION_FUNCTION = None\n",
    "\n",
    "# Cycle Consistent GAN constants\n",
    "KERNEL_SIZE = 4\n",
    "STRIDE = 2\n",
    "PADDING = 1\n",
    "OUT_CHANNELS = 1\n",
    "IN_CHANNELS = 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12efa7bdba9fe235",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:39.070480700Z",
     "start_time": "2023-08-01T16:58:39.045273Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Defining all hyperparameters\n",
    "##############################################\n",
    "class Hyperparameters(object):\n",
    "    \"\"\"\n",
    "        This class contains the hyperparameters for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        self.epoch = None,\n",
    "        self.n_epochs = None,\n",
    "        self.dataset_train_mode = None,\n",
    "        self.dataset_test_mode = None,\n",
    "        self.batch_size = None,\n",
    "        self.lr = None,\n",
    "        self.decay_start_epoch = None,\n",
    "        self.b1 = None,\n",
    "        self.b2 = None,\n",
    "        self.n_cpu = None,\n",
    "        self.img_size = None,\n",
    "        self.channels = None,\n",
    "        self.n_critic = None,\n",
    "        self.sample_interval = None,\n",
    "        self.num_residual_blocks = None,\n",
    "        self.lambda_cyc = None,\n",
    "        self.lambda_id = None,\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "###############################################\n",
    "# create an instance of Hyperparameters\n",
    "###############################################\n",
    "\n",
    "hp = Hyperparameters(\n",
    "    epoch=0,\n",
    "    n_epochs=200,\n",
    "    dataset_train_mode=\"train\",\n",
    "    dataset_test_mode=\"test\",\n",
    "    batch_size=4,\n",
    "    lr=0.0002,\n",
    "    decay_start_epoch=100,\n",
    "    b1=0.5,\n",
    "    b2=0.999,\n",
    "    n_cpu=8,\n",
    "    img_size=128,\n",
    "    channels=3,\n",
    "    n_critic=5,\n",
    "    sample_interval=100,\n",
    "    num_residual_blocks=19,\n",
    "    lambda_cyc=10.0,\n",
    "    lambda_id=5.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4731a614d2bd5ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:41.342833500Z",
     "start_time": "2023-08-01T16:58:41.309063200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_rgb(image):\n",
    "    \"\"\"\n",
    "        Convert image to RGB\n",
    "    \"\"\"\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image\n",
    "\n",
    "\n",
    "########################################################\n",
    "# Methods for Image Visualization\n",
    "########################################################\n",
    "\n",
    "def show_img(img, size=10):\n",
    "    \"\"\"\n",
    "        Show image\n",
    "    \"\"\"\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    \"\"\"\n",
    "        Convert tensor to image\n",
    "    \"\"\"\n",
    "    x = x.view(x.size(0) * 2, hp.channels, hp.img_size, hp.img_size)\n",
    "    return x\n",
    "\n",
    "\n",
    "def plot_output(path, x, y):\n",
    "    \"\"\"\n",
    "        Plot the output\n",
    "    \"\"\"\n",
    "    img = mpimg.imread(path)\n",
    "    plt.figure(figsize=(x, y))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Custom Image Dataset class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir,\n",
    "        transforms_=None,\n",
    "        unaligned=False,\n",
    "        mode=\"train\",\n",
    "        set_a=\"A\",\n",
    "        set_b=\"B\",\n",
    "    ):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        # self.files_a = sorted(\n",
    "        #     glob.glob(os.path.join(root_dir, \"%strainA\" % mode) + \"/*.*\"))\n",
    "        # self.files_b = sorted(\n",
    "        #     glob.glob(os.path.join(root_dir, \"%strainB\" % mode) + \"/*.*\"))\n",
    "\n",
    "        self.files_a = sorted(\n",
    "            glob.glob(os.path.join(root_dir, f\"{mode}{set_a}\", \"*.*\"))\n",
    "        )\n",
    "        self.files_b = sorted(\n",
    "            glob.glob(os.path.join(root_dir, f\"{mode}{set_b}\", \"*.*\"))\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_a = Image.open(self.files_a[index % len(self.files_a)])\n",
    "        # a % b => a is divided by b, and the remainder of that division is returned.\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_b = Image.open(\n",
    "                self.files_b[random.randint(0, len(self.files_b) - 1)])\n",
    "        else:\n",
    "            image_b = Image.open(self.files_b[index % len(self.files_b)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_a.mode != \"RGB\":\n",
    "            image_a = convert_to_rgb(image_a)\n",
    "        if image_b.mode != \"RGB\":\n",
    "            image_b = convert_to_rgb(image_b)\n",
    "\n",
    "        item_a = self.transform(image_a)\n",
    "        item_b = self.transform(image_b)\n",
    "\n",
    "        # Finally return a dict\n",
    "        return {\n",
    "            \"A\": item_a,\n",
    "            \"B\": item_b\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_a), len(self.files_b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c5344126fb8089",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Bonus Point - How does a % b works when a is smaller than b\n",
    "\n",
    "<a href=\"https://stackoverflow.com/questions/1535656/how-does-a-modulo-operation-work-when-the-first-number-is-smaller\" >read more here</a>\n",
    "\n",
    "for instance\n",
    "\n",
    "2 % 5 the answer is 2.\n",
    "\n",
    "2 divided by 5 (integer division) is 0 with a remainder of 2.\n",
    "\n",
    "2 = 0 x 5 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39bfb372a19a0802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:45.001489500Z",
     "start_time": "2023-08-01T16:58:44.942121600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2 % 5)\n",
    "print(2 % 8)\n",
    "print(2 % 15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc562f210492546d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get Train and Validation Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44395325fed525d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:58:47.397486400Z",
     "start_time": "2023-08-01T16:58:47.326309100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(\n",
    "        (hp.img_size, hp.img_size),\n",
    "        interpolation=InterpolationMode.BICUBIC\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    ImageDataset(\n",
    "        DATASET_ROOT_DIR,\n",
    "        mode=DATASET_TRAIN_MODE,\n",
    "        transforms_=transforms_\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\n",
    "        DATASET_ROOT_DIR,\n",
    "        mode=DATASET_TEST_MODE,\n",
    "        transforms_=transforms_\n",
    "    ),\n",
    "    batch_size=VAL_BATCH_SIZE,\n",
    "    shuffle=SHUFFLE,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36a69e72e9fbc35",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Replay Buffer \n",
    "\n",
    "As per the paper -  To reduce model oscillation, we update the discriminator using a history of generated images rather than the ones produced by the latest generators. We keep an image buffer that stores the 50 previously created images.\n",
    "\n",
    "And here is the link the Paper Published in 2017 by Shrivastava - https://arxiv.org/pdf/1612.07828.pdf\n",
    "\n",
    "\n",
    "This is another strategy used to stabilize the CycleGAN Training\n",
    "\n",
    "Replay buffer is used to train the discriminator. Generated images are added to the replay buffer and sampled from it.\n",
    "\n",
    "The replay buffer returns the newly added image with a probability of 0.5. \n",
    "\n",
    "Otherwise, it sends an older generated image and replaces the older image with the newly generated image.\n",
    "\n",
    "This is done to reduce model oscillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d97232a4c5d560e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:10.823960200Z",
     "start_time": "2023-08-01T16:59:10.800913300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# # # # # Replay Buffer\n",
    "###############################################################################\n",
    "\n",
    "class ReplayBuffer:\n",
    "    # We keep an image buffer that stores\n",
    "    # the 50 previously created images.\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                # If the buffer is full, decide whether to replace\n",
    "                # an old image in the buffer with the new one,\n",
    "                # and whether to add the new image or an old image to the return batch.\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    # With a 50% chance, replace an old image in the buffer with the new image\n",
    "                    # and add the old image to the return batch.\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[\n",
    "                        i\n",
    "                    ] = element  # replaces the older image with the newly generated image.\n",
    "                else:\n",
    "                    # With a 50% chance, keep the buffer as is and\n",
    "                    # add the new image to the return batch.\n",
    "                    to_return.append(element)\n",
    "        return Variable(torch.cat(to_return))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551137ba45d523",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Learning Rate scheduling with `lr_lambda`\n",
    "\n",
    "\"We keep the same learning rate\n",
    "for the first 100 epochs and linearly decay the rate to zero\n",
    "over the next 100 epochs.\"\n",
    "\n",
    "### First, I am creating a class `LambdaLR(n_epochs, offset, decay_start_epoch)` - Lets understand how its working\n",
    "\n",
    "Following the Paper, in my `LambdaLR` class the `decay_start_epoch` hyperparameter is kept at 100\n",
    "\n",
    "And then just before training, I will invoke the `LambdaLR()` method as below, to set the `lr_scheduler_G`\n",
    "\n",
    "```py\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
    ")\n",
    "```\n",
    "\n",
    "Where `lr_lambda` (is a function or list) – A function which computes a multiplicative factor given an integer parameter epoch, or a list of such functions, one for each group in optimizer.param_groups.\n",
    "\n",
    "So basically, below ia a simplified application of the lambda function.\n",
    "\n",
    "```py\n",
    "lambda_func = lambda epoch: 1 - max(0, epoch - decay_start_epoch)/(n_epochs - decay_start_epoch)\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_func)\n",
    "```\n",
    "\n",
    "### Understanding `lr_lambda` arg in `torch.optim.lr_scheduler.LambdaLR`\n",
    "\n",
    "lr_lambda Sets the learning rate of each parameter group to the initial lr times a given function. When last_epoch=-1, sets initial lr as lr.\n",
    "\n",
    "The new learning rate is always calculated like that:\n",
    "\n",
    "### lr_epoch = lr_initial ∗ Lambda(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45b877b7a3a18da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:13.490156200Z",
     "start_time": "2023-08-01T16:59:13.441479800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Learning Rate scheduling with `lr_lambda`\n",
    "########################################################\n",
    "\n",
    "\n",
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (\n",
    "                       n_epochs - decay_start_epoch\n",
    "               ) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (\n",
    "                self.n_epochs - self.decay_start_epoch\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369d4b2b847a4da",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### So e.g. for epoch=110, the above function `LambdaLR` will return as below\n",
    "\n",
    "`return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)`\n",
    "\n",
    "will be\n",
    "\n",
    "1.0 - max(0, 110 + 0 - 100) / (200 - 100) \n",
    "\n",
    "= 1.0 - max(0, 10) / (100) \n",
    "\n",
    "= 1 - 1/10 = 0.9\n",
    "\n",
    "So that means the Decay Factor of the Learning rate for epoch=110 is 0.9 i.e. the Learning rate would be reduced as below\n",
    "\n",
    "New LR = Initial LR * 0.9\n",
    "\n",
    "### Similarly for epoch=120, `LambdaLR` will return as below\n",
    "\n",
    "1.0 - max(0, 120 + 0 - 100) / (200 - 100) \n",
    "\n",
    "= 1.0 - max(0, 20) / (100) \n",
    "\n",
    "= 1 - 1/5 = 0.8\n",
    "\n",
    "So that means the Decay Factor of the Learning rate for epoch=110 is 0.9 i.e. the Learning rate would be reduced as below\n",
    "\n",
    "New LR = Initial LR * 0.8\n",
    "\n",
    "--------------------------------------\n",
    "\n",
    "#### Overall, in my `LambdaLR` class - the implementation logic goes like this\n",
    "\n",
    "- In-order to linearly decay the learning rate after 100 epoch the lambda function checks whether the current epoch has exceeds the `decay_start_epoch`(which is 100).\n",
    "\n",
    "- If current epoch is less than `decay_start_epoch`(which is 100) it returns 1. So that initial lr remain same for the first 100 epochs.\n",
    "\n",
    "- If the current epoch has exceeded the `decay_start_epoch`(which is 100) the initial lr will be decreased through out the rest of the epochs among total epochs(rest 100 epoch out of total 200 epochs of training).\n",
    "\n",
    "- If we equally divide the lr for the last 100 epochs and keep subtracting from \"Base-LR\" or \"Initial LR\", it will reach to 0 by the end of the last 100 epochs.\n",
    "\n",
    "- As lambda lr multiply initial lr with given function, epoch beyond the `decay_start_epoch` will sum up the consistent decrease in lr value from staring of decay epoch(which is 100) to the current epoch(for example 110)).\n",
    "\n",
    "As it does not have the decayed lr at previous epochs(here epoch 109 in case of current epoch 110) and only have Base lr or the Initial LR, it sum up the decrement occurred in lr for the previous epoch.\n",
    "\n",
    "--------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a3f4875786f94b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Initialize convolution layer weights to N(0,0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dbec15c904c6363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:17.578282900Z",
     "start_time": "2023-08-01T16:59:17.557556700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_conv_weights_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa097d2b9a387e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GENERATOR & DISCRIMINATOR\n",
    "\n",
    "### Residual Block\n",
    "\n",
    "Reflection padding was used to reduce artifacts.\n",
    "A residual block contains two 3 × 3 convolutional layers with the same number of filters on both layer.\n",
    "\n",
    "\n",
    "<blockquote>\n",
    "<p>**7.2. Network architectures** -->Generator architectures -->\"We use 6 residual blocks for 128 × 128 training images, and 9 residual blocks for 256 × 256 or higher-resolution training images.\"\n",
    "\n",
    "\"Let c7s1-k denote a 7×7 Convolution-InstanceNormReLU layer with k filters and stride 1. dk denotes a 3 × 3 Convolution-InstanceNorm-ReLU layer with k filters and stride 2. Reflection padding was used to reduce artifacts. Rk denotes a residual block that contains two 3 × 3 convolutional layers with the same number of filters on both layer. uk denotes a 3 × 3 fractional-strided-ConvolutionInstanceNorm-ReLU layer with k filters.\"\n",
    "\n",
    "\"The network with 9 residual blocks consists of:\n",
    "**\"c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,u128 u64,c7s1-3\"**</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d818e61338bf90f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:20.140236400Z",
     "start_time": "2023-08-01T16:59:20.099458300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Residual block with two convolution layers.\n",
    "##############################################\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "        Residual block class implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(\n",
    "                1\n",
    "            ),  # Reflection padding is used because it gives better image quality at edges.\n",
    "            nn.Conv2d(\n",
    "                in_channel, in_channel, 3\n",
    "            ),  # Paper says - same number of filters on both layer.\n",
    "            nn.InstanceNorm2d(in_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channel, in_channel, 3),\n",
    "            nn.InstanceNorm2d(in_channel),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass of the residual block\n",
    "        \"\"\"\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe54d1dc33087b10",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Parameters in torch.nn.conv2d()\n",
    "\n",
    "```py\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "\n",
    "```\n",
    "\n",
    "Where\n",
    "\n",
    "* in_channels (int) – Number of channels/filters in the input image\n",
    "\n",
    "* out_channels (int) – Number of channels/filters produced by the convolution\n",
    "\n",
    "* kernel_size (int or tuple) – Size of the convolving kernel\n",
    "\n",
    "* stride (int or tuple, optional) – Stride of the convolution. (Default: 1)\n",
    "\n",
    "* padding (int or tuple, optional) – Zero-padding added to both sides of the input (Default: 0)\n",
    "\n",
    "* padding_mode (string, optional) – zeros\n",
    "\n",
    "* dilation (int or tuple, optional) – Spacing between kernel elements. (Default: 1)\n",
    "\n",
    "* groups (int, optional) – Number of blocked connections from input to output channels. (Default: 1)\n",
    "\n",
    "* bias (bool, optional) – If True, adds a learnable bias to the output. (Default: True)\n",
    "\n",
    "---\n",
    "\n",
    "## The generator\n",
    "\n",
    "* The generator consists encoder and decoder. It downsample or encode the input image, then interpret the encoding with 9 Residual Blocks having skip connections.After that with a a series of layers it upsample or decode the representation to the size of the fake image.\n",
    "\n",
    "* Reflection padding “reflects” the row into the padding. It is used mostly for brightness, contrast and for reducing artifact.\n",
    "\n",
    "* Batch norm normalizes across the mini batch of definite size.On the other hand, Instance normalization normalizes across each channel in each data instead of normalizing across input features in a data. Instance Norm normalizes each batch independently and across spatial locations only.\n",
    "\n",
    "* Use of instance normalization layers, the normalization process allows to remove instance-specific contrast information from the image content, which simplifies image generation. Thus results in vastly improved images.\n",
    "\n",
    "![Imgur](https://imgur.com/38kq2bw.png)\n",
    "\n",
    "As you can see above, the representation size shrinks in the encoder phase, stays constant in the transformer phase, and expands again in the decoder phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19f391a82cfc1ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:22.582423800Z",
     "start_time": "2023-08-01T16:59:22.548649300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Generator\n",
    "##############################################\n",
    "# Generator with 9 residual blocks consists of: -> ()\n",
    "# c7s1-64,d128,d256,R256,R256,R256, R256,R256,R256,R256,R256,R256,\n",
    "# u128, u64,c7s1-3\n",
    "##############################################\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    \"\"\"\n",
    "        Generator model class implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape, num_residual_blocks):\n",
    "        \"\"\"\n",
    "            Initialize the generator model for the cycle consistent GAN\n",
    "        \"\"\"\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_channels = 64\n",
    "        # define a variable 'model' which will continue to update\n",
    "        # throughout the 3 blocks of Residual -> Downsampling -> Upsampling\n",
    "        # First c7s1-64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_channels, kernel_size=7),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_channels = out_channels\n",
    "\n",
    "        # Down-sampling\n",
    "        # d128 => d256\n",
    "        for _ in range(2):\n",
    "            out_channels *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=3, stride=2, padding=1\n",
    "                ),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Residual blocks\n",
    "        # R256,R256,R256,R256,R256,R256,R256,R256,R256\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_channels)]\n",
    "\n",
    "        # Up-sampling\n",
    "        # u128 => u64\n",
    "        for _ in range(2):\n",
    "            out_channels //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "                ),\n",
    "                nn.InstanceNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "\n",
    "        # Output layer\n",
    "        # c7s1-3\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(out_channels, channels, 7),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Forward pass of the generator model\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d4b7ed197b30",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Meaning for *model in `nn.Sequential(*model)` \n",
    "\n",
    "The syntax is to use the symbol * to take in a variable number of arguments\n",
    "\n",
    "*args allows you to do is take in more arguments than the number of formal arguments that you previously defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a0dc70806e7441",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Discriminator\n",
    "\n",
    "As per the Paper :\n",
    "\n",
    "<blockquote>\n",
    "<p>\n",
    "\n",
    "**7.2. Network architectures - Discriminator architectures**\n",
    "\n",
    "\"For discriminator networks, we use 70 × 70 PatchGAN [22]. Let Ck denote a 4 × 4 Convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2. After the last layer, we apply a convolution to produce a 1-dimensional output. We do not use InstanceNorm for the first C64 layer. We use leaky ReLUs with a slope of 0.2. The discriminator architecture is:\" C64-C128-C256-C512\n",
    "<p/>\n",
    "<blockquote/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eddedbba5f35d8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The model also has a final hidden layer C512 with a 1×1 stride. \n",
    "\n",
    "Given the model is mostly used with 256×256 sized images as input, the size of the output feature map of activations is 16×16. If 128×128 images were used as input, then the size of the output feature map of activations would be 8×8.\n",
    "\n",
    "![Imgur](https://imgur.com/ti5xUof.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b3d84336e76bfae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:35.264750300Z",
     "start_time": "2023-08-01T16:59:35.231703500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "####################\n",
    "# #   Discriminator\n",
    "####################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Discriminator model class implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_channels, out_channels, normalize=True):\n",
    "            layers = [\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=KERNEL_SIZE,\n",
    "                    stride=STRIDE,\n",
    "                    padding=1\n",
    "                )\n",
    "            ]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_channels))\n",
    "            layers.append(\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, out_channels=64, normalize=False),\n",
    "            *discriminator_block(64, out_channels=128),\n",
    "            *discriminator_block(128, out_channels=256),\n",
    "            *discriminator_block(256, out_channels=512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(\n",
    "                # Update this to the correct number of output channels from the last discriminator_block\n",
    "                in_channels=512,\n",
    "                out_channels=1,\n",
    "                kernel_size=KERNEL_SIZE,\n",
    "                padding=PADDING\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486ae83ecdcb23f6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The main hyperparameters for the discriminator are, namely, number of output filters, kernel size and stride. A trivial configuration is shown below. Further tuning is needed when training the model.\n",
    "\n",
    "![Imgur](https://imgur.com/Ia7lyc2.png)\n",
    "\n",
    "We also use padding to maintain the information of pixels on the boundary of the image.\n",
    "\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bdce6c795adffb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## So where is PatchGAN Implemented above.\n",
    "\n",
    "Ans is PatchGAN is in-built in the very structure in above.\n",
    "\n",
    "First, noting again that **The main difference between a PatchGAN and a regular GAN discriminator is that - the regular GAN maps an input image to a single scalar output in the range of [0,1], indicating the probability of the image being real or fake, while PatchGAN provides Matrix as the output with each entry signifying whether its corresponding patch is real or fake.**\n",
    "\n",
    "The architecture for our Discriminator here, is as follows:\n",
    "\n",
    "### C64 => C128 => C256 => C512\n",
    "\n",
    "This is referred to as a 3-layer PatchGAN in the CycleGAN and Pix2Pix nomenclature, as excluding the first hidden layer, the model has three hidden layers that could be scaled up or down to give different sized PatchGAN models.\n",
    "\n",
    "In PatchGAN, given for example image of size 256x256, the PatchGAN maps from that 256x256 to an NxN Matrix of outputs X, where each `X_ij` of that NxN Matrix signifies whether the patch `ij` (in X) in the image is real or fake. So each of this `X_ij` value (which is a single scaler value) is a probability for the likelihood that a patch in the input image is real.\n",
    "\n",
    "We can test it with below code, that just calculates the output shape of the Matrix given to a PatchGAN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8fd9e6dc310cfb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:39.991395400Z",
     "start_time": "2023-08-01T16:59:39.942444100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fourth_layer  7\n",
      "third_layer_input_size  16\n",
      "second_layer_input_size  34\n",
      "first_layer_input_size  70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Receptive field: 70'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_input_size(output_size, filter_size, stride):\n",
    "    return (output_size - 1) * stride + filter_size\n",
    "\n",
    "# Now invoke above method to calculate the size of various layers in \n",
    "# Discriminator Network\n",
    "\n",
    "last_layer = get_input_size(output_size=1, filter_size=4, stride=1)\n",
    "\n",
    "fourth_layer_input_size = get_input_size(output_size=last_layer, filter_size=4, stride=1)\n",
    "print(\"fourth_layer \", fourth_layer_input_size)\n",
    "\n",
    "\"\"\"Receptive field: 7\"\"\"\n",
    "third_layer_input_size = get_input_size(output_size=fourth_layer_input_size, filter_size=4, stride=2)\n",
    "print(\"third_layer_input_size \", third_layer_input_size)\n",
    "\n",
    "\"\"\"Receptive field: 16\"\"\"\n",
    "second_layer_input_size = get_input_size(output_size=third_layer_input_size, filter_size=4, stride=2)\n",
    "print('second_layer_input_size ', second_layer_input_size)\n",
    "\n",
    "\"\"\"Receptive field: 34\"\"\"\n",
    "first_layer_input_size = get_input_size(output_size=second_layer_input_size, filter_size=4, stride=2)\n",
    "print('first_layer_input_size ', first_layer_input_size)\n",
    "\n",
    "\"\"\"Receptive field: 70\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a5c9ea412bd2bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:56.262025400Z",
     "start_time": "2023-08-01T16:59:54.522569400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using CUDA\n"
     ]
    }
   ],
   "source": [
    "# SETUP, LOSS, INITIALIZE MODELS and BUFFERS\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(\"Using CUDA\" if cuda else \"Not using CUDA\")\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
    "\n",
    "##############################################\n",
    "# SETUP, LOSS, INITIALIZE MODELS and BUFFERS\n",
    "##############################################\n",
    "\n",
    "# Creating criterion object (Loss Function) that will\n",
    "# measure the error between the prediction and the target.\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "\n",
    "criterion_identity = torch.nn.L1Loss()\n",
    "\n",
    "input_shape = (hp.channels, hp.img_size, hp.img_size)\n",
    "\n",
    "##############################################\n",
    "# Initialize generator and discriminator\n",
    "##############################################\n",
    "\n",
    "Gen_AB = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
    "Gen_BA = GeneratorResNet(input_shape, hp.num_residual_blocks)\n",
    "\n",
    "Disc_A = Discriminator(input_shape)\n",
    "Disc_B = Discriminator(input_shape)\n",
    "\n",
    "if cuda:\n",
    "    Gen_AB = Gen_AB.cuda()\n",
    "    Gen_BA = Gen_BA.cuda()\n",
    "    Disc_A = Disc_A.cuda()\n",
    "    Disc_B = Disc_B.cuda()\n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()\n",
    "\n",
    "##############################################\n",
    "# Initialize weights\n",
    "##############################################\n",
    "\n",
    "Gen_AB.apply(initialize_conv_weights_normal)\n",
    "Gen_BA.apply(initialize_conv_weights_normal)\n",
    "\n",
    "Disc_A.apply(initialize_conv_weights_normal)\n",
    "Disc_B.apply(initialize_conv_weights_normal)\n",
    "\n",
    "##############################################\n",
    "# Buffers of previously generated samples\n",
    "##############################################\n",
    "\n",
    "fake_a_buffer = ReplayBuffer()\n",
    "\n",
    "fake_b_buffer = ReplayBuffer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f5178c4605b1d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T16:59:58.482852400Z",
     "start_time": "2023-08-01T16:59:58.439044500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# SAMPLING IMAGES\n",
    "##############################################\n",
    "\n",
    "\n",
    "def save_img_samples(batches_done):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    print(\"batches_done \", batches_done)\n",
    "    imgs = next(iter(val_dataloader))\n",
    "\n",
    "    Gen_AB.eval()\n",
    "    Gen_BA.eval()\n",
    "\n",
    "    real_a = Variable(imgs[\"A\"].type(Tensor))\n",
    "    fake_b = Gen_AB(real_a)\n",
    "    real_b = Variable(imgs[\"B\"].type(Tensor))\n",
    "    fake_a = Gen_BA(real_b)\n",
    "    # Arrange images along x-axis\n",
    "    real_a = make_grid(real_a, nrow=16, normalize=True)\n",
    "    real_b = make_grid(real_b, nrow=16, normalize=True)\n",
    "    fake_a = make_grid(fake_a, nrow=16, normalize=True)\n",
    "    fake_b = make_grid(fake_b, nrow=16, normalize=True)\n",
    "    # Arrange images along y-axis\n",
    "    image_grid = torch.cat((real_a, fake_b, real_b, fake_a), 1)\n",
    "\n",
    "    path = SAMPLES_DIR + \"/%s.png\" % batches_done\n",
    "\n",
    "    save_image(image_grid, path, normalize=False)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ef34cf8279ad7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a4f7f816ee26d6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T17:00:06.092198400Z",
     "start_time": "2023-08-01T17:00:06.072120400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Defining all Optimizers\n",
    "##############################################\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(Gen_AB.parameters(), Gen_BA.parameters()),\n",
    "    lr=hp.lr,\n",
    "    betas=(hp.b1, hp.b2),\n",
    ")\n",
    "optimizer_Disc_A = torch.optim.Adam(\n",
    "    Disc_A.parameters(),\n",
    "    lr=hp.lr,\n",
    "    betas=(hp.b1, hp.b2)\n",
    ")\n",
    "\n",
    "optimizer_Disc_B = torch.optim.Adam(\n",
    "    Disc_B.parameters(),\n",
    "    lr=hp.lr,\n",
    "    betas=(hp.b1, hp.b2)\n",
    ")\n",
    "\n",
    "##############################################\n",
    "# Learning rate update schedulers\n",
    "##############################################\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step\n",
    ")\n",
    "\n",
    "lr_scheduler_Disc_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_Disc_A,\n",
    "    lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step,\n",
    ")\n",
    "lr_scheduler_Disc_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_Disc_B,\n",
    "    lr_lambda=LambdaLR(hp.n_epochs, hp.epoch, hp.decay_start_epoch).step,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a1d3918d7dea13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T17:00:07.590331Z",
     "start_time": "2023-08-01T17:00:07.565118800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        Gen_BA,\n",
    "        Gen_AB,\n",
    "        Disc_A,\n",
    "        Disc_B,\n",
    "        train_dataloader,\n",
    "        n_epochs,\n",
    "        criterion_identity,\n",
    "        criterion_cycle,\n",
    "        lambda_cyc,\n",
    "        criterion_GAN,\n",
    "        optimizer_G,\n",
    "        fake_a_buffer,\n",
    "        fake_b_buffer,\n",
    "        clear_output,\n",
    "        optimizer_Disc_A,\n",
    "        optimizer_Disc_B,\n",
    "        Tensor,\n",
    "        sample_interval,\n",
    "        lambda_id,\n",
    "):\n",
    "    # TRAINING\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(hp.epoch, n_epochs):\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_a = Variable(batch[\"A\"].type(Tensor))\n",
    "            real_b = Variable(batch[\"B\"].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths i.e. target vectors\n",
    "            # 1 for real images and 0 for fake generated images\n",
    "            valid = Variable(\n",
    "                Tensor(np.ones((real_a.size(0), *Disc_A.output_shape))),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "\n",
    "            fake = Variable(\n",
    "                Tensor(np.zeros((real_a.size(0), *Disc_A.output_shape))),\n",
    "                requires_grad=False,\n",
    "            )\n",
    "\n",
    "            ###################################\n",
    "            # # Train Generators A->B and B->A\n",
    "            ###################################\n",
    "\n",
    "            Gen_AB.train()\n",
    "            Gen_BA.train()\n",
    "\n",
    "            \"\"\"\n",
    "            PyTorch stores gradients in a mutable data structure. \n",
    "            So we need to set it to a clean state before we use it.\n",
    "            Otherwise, it will have old information from a previous iteration.\n",
    "            \"\"\"\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Identity loss\n",
    "            # First pass real_a images to the Generator, that will generate A-domains images\n",
    "            loss_id_A = criterion_identity(Gen_BA(real_a), real_a)\n",
    "\n",
    "            # Then pass real_b images to the Generator, that will generate B-domains images\n",
    "            loss_id_B = criterion_identity(Gen_AB(real_b), real_b)\n",
    "\n",
    "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
    "\n",
    "            # GAN losses for GAN_AB\n",
    "            fake_b = Gen_AB(real_a)\n",
    "\n",
    "            loss_GAN_AB = criterion_GAN(Disc_B(fake_b), valid)\n",
    "\n",
    "            # GAN losses for GAN_BA\n",
    "            fake_a = Gen_BA(real_b)\n",
    "\n",
    "            loss_GAN_BA = criterion_GAN(Disc_A(fake_a), valid)\n",
    "\n",
    "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "\n",
    "            # Cycle Consistency losses\n",
    "            reconstructed_A = Gen_BA(fake_b)\n",
    "\n",
    "            \"\"\"\n",
    "            Forward Cycle Consistency Loss\n",
    "            Forward cycle loss:  lambda * ||G_BtoA(G_AtoB(A)) - A|| (Equation 2 in the paper)\n",
    "            Compute the cycle consistency loss by comparing the reconstructed_A \n",
    "            images with real real_a  images of domain A.\n",
    "            Lambda for cycle loss is 10.0. Penalizing 10 times and forcing to learn the translation.\n",
    "            \"\"\"\n",
    "            loss_cycle_A = criterion_cycle(reconstructed_A, real_a)\n",
    "\n",
    "            \"\"\"\n",
    "            Backward Cycle Consistency Loss\n",
    "            Backward cycle loss: lambda * ||G_AtoB(G_BtoA(B)) - B|| (Equation 2 of the Paper)\n",
    "            Compute the cycle consistency loss by comparing the reconstructed_B \n",
    "            images with real real_b images of domain B.\n",
    "            Lambda for cycle loss is 10.0. Penalizing 10 times and forcing to learn the translation.\n",
    "            \"\"\"\n",
    "            reconstructed_B = Gen_AB(fake_a)\n",
    "\n",
    "            loss_cycle_B = criterion_cycle(reconstructed_B, real_b)\n",
    "\n",
    "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "            \"\"\"\n",
    "            Finally, Total Generators Loss and Back propagation\n",
    "            Add up all the Generators loss and cyclic loss (Equation 3 of paper.\n",
    "            Also Equation I the code representation of the equation) and perform backpropagation with optimization.\n",
    "            \"\"\"\n",
    "            loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n",
    "\n",
    "            loss_G.backward()\n",
    "\n",
    "            \"\"\"\n",
    "            Now we just need to update all the parameters!\n",
    "            Θ_{k+1} = Θ_k - η * ∇_Θ ℓ(y_hat, y)\n",
    "            \"\"\"\n",
    "            optimizer_G.step()\n",
    "\n",
    "            #########################\n",
    "            #  Train Discriminator A\n",
    "            #########################\n",
    "\n",
    "            optimizer_Disc_A.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(Disc_A(real_a), valid)\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "\n",
    "            fake_a_ = fake_a_buffer.push_and_pop(fake_a)\n",
    "\n",
    "            loss_fake = criterion_GAN(Disc_A(fake_a_.detach()), fake)\n",
    "\n",
    "            \"\"\" Total loss for Disc_A\n",
    "            And I divide by 2 because as per Paper - \"we divide the objective by 2 while\n",
    "            optimizing D, which slows down the rate at which D learns,\n",
    "            relative to the rate of G.\"\n",
    "            \"\"\"\n",
    "            loss_Disc_A = (loss_real + loss_fake) / 2\n",
    "\n",
    "            \"\"\" do backpropagation i.e.\n",
    "            ∇_Θ will get computed by this call below to backward() \"\"\"\n",
    "            loss_Disc_A.backward()\n",
    "\n",
    "            \"\"\"\n",
    "            Now we just need to update all the parameters!\n",
    "            Θ_{k+1} = Θ_k - η * ∇_Θ ℓ(y_hat, y)\n",
    "            \"\"\"\n",
    "            optimizer_Disc_A.step()\n",
    "\n",
    "            #########################\n",
    "            #  Train Discriminator B\n",
    "            #########################\n",
    "\n",
    "            optimizer_Disc_B.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = criterion_GAN(Disc_B(real_b), valid)\n",
    "\n",
    "            # Fake loss (on batch of previously generated samples)\n",
    "            fake_b_ = fake_b_buffer.push_and_pop(fake_b)\n",
    "\n",
    "            loss_fake = criterion_GAN(Disc_B(fake_b_.detach()), fake)\n",
    "\n",
    "            \"\"\" Total loss for Disc_B\n",
    "            And I divide by 2 because as per Paper - \"we divide the objective by 2 while\n",
    "            optimizing D, which slows down the rate at which D learns,\n",
    "            relative to the rate of G.\"\n",
    "            \"\"\"\n",
    "            loss_Disc_B = (loss_real + loss_fake) / 2\n",
    "\n",
    "            \"\"\" do backpropagation i.e.\n",
    "            ∇_Θ will get computed by this call below to backward() \"\"\"\n",
    "            loss_Disc_B.backward()\n",
    "\n",
    "            \"\"\"\n",
    "            Now we just need to update all the parameters!\n",
    "            Θ_{k+1} = Θ_k − η * ∇_Θ ℓ(y_hat, y)\n",
    "            \"\"\"\n",
    "            optimizer_Disc_B.step()\n",
    "\n",
    "            loss_D = (loss_Disc_A + loss_Disc_B) / 2\n",
    "\n",
    "            ##################\n",
    "            #  Log Progress\n",
    "            ##################\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_dataloader) + i\n",
    "\n",
    "            batches_left = n_epochs * len(train_dataloader) - batches_done\n",
    "\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time)\n",
    "            )\n",
    "            prev_time = time.time()\n",
    "\n",
    "            print(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, cycle: %f, identity: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    n_epochs,\n",
    "                    i,\n",
    "                    len(train_dataloader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    loss_cycle.item(),\n",
    "                    loss_identity.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                clear_output()\n",
    "                plot_output(save_img_samples(batches_done), 30, 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d801fd061c826e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-08-01T17:00:27.714153600Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Execute the Final Training Function\n",
    "##############################################\n",
    "\n",
    "train(\n",
    "    Gen_BA=Gen_BA,\n",
    "    Gen_AB=Gen_AB,\n",
    "    Disc_A=Disc_A,\n",
    "    Disc_B=Disc_B,\n",
    "    train_dataloader=train_dataloader,\n",
    "    n_epochs=hp.n_epochs,\n",
    "    criterion_identity=criterion_identity,\n",
    "    criterion_cycle=criterion_cycle,\n",
    "    lambda_cyc=hp.lambda_cyc,\n",
    "    criterion_GAN=criterion_GAN,\n",
    "    optimizer_G=optimizer_G,\n",
    "    fake_a_buffer=fake_a_buffer,\n",
    "    fake_b_buffer=fake_b_buffer,\n",
    "    clear_output=clear_output,\n",
    "    optimizer_Disc_A=optimizer_Disc_A,\n",
    "    optimizer_Disc_B=optimizer_Disc_B,\n",
    "    Tensor=Tensor,\n",
    "    sample_interval=hp.sample_interval,\n",
    "    lambda_id=hp.lambda_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2168de2bae556",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
